{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gICjvO5W5A2n"},"outputs":[],"source":["import pandas as pd\n","import os\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torchvision.utils as vutils\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZRZ4CXJ55pH"},"outputs":[],"source":["# Hyperparameters\n","\n","batch_size = 64 # Batch size during training\n","image_size = 128  # Images are 128x128(resized)\n","image_channels = 3  # RGB images\n","ngpu=1 #number of GPUs to use\n","z_dim = 100  # Size/dimension of the latent vector (input noise)\n","gen_features = 64  # Feature map size of Generator\n","dis_features = 64  # Feature map size Discriminator\n","epochs = 100  # Number of epochs during train\n","lr = 0.0002  # Learning rate\n","beta1 = 0.5  # Beta1-A hyperparameter in Adam optimizer\n","beta2= 0.999 # Beta2-A hyperparameter in for Adam optimizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":589,"status":"ok","timestamp":1728020480751,"user":{"displayName":"Viking Chaitu","userId":"15693266827587795521"},"user_tz":-330},"id":"x3H91XPl6KMx","outputId":"bca0ec9e-23cb-4aba-d14b-cc447f9c2c89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":3}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","source":["Question 1:\n","\n","Train a DC GAN (with an architecture of your choice) on the given data\n","with the usual GAN loss. Plot the loss curves for the Generator and\n","Discriminator losses"],"metadata":{"id":"iU-KGE5CXosW"}},{"cell_type":"markdown","source":["Generator for DCGAN"],"metadata":{"id":"TrrghoR6Xv93"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIr0Fq1X5D7Z"},"outputs":[],"source":["'''\n","   Takes input as latent vector(sampled from normal distribution)\n","   this input passed through series of transpose conv,batch norm and activation function\n","   layers for upsampling.\n","   Gives a newly generated images as output\n","'''\n","\n","class dcgan_Generator(nn.Module):\n","    def __init__(self, z_dim, image_channels,gen_features):\n","        super(dcgan_Generator, self).__init__()\n","        self.generator_layers=nn.Sequential(\n","            nn.ConvTranspose2d(in_channels=z_dim,out_channels=gen_features*16,kernel_size=4,stride=1,padding=0,bias=False),\n","            nn.BatchNorm2d(gen_features*16),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=gen_features*16,out_channels=gen_features*8,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.BatchNorm2d(gen_features*8),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=gen_features*8,out_channels=gen_features*4,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.BatchNorm2d(gen_features*4),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=gen_features*4,out_channels=gen_features*2,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.BatchNorm2d(gen_features*2),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=gen_features*2,out_channels=gen_features,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.BatchNorm2d(gen_features),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=gen_features,out_channels=image_channels,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.Tanh() #scales pixel values(-1 to 1)\n","\n","        )\n","\n","\n","    def forward(self,x):\n","        return self.generator_layers(x)\n"]},{"cell_type":"markdown","source":["Discriminator for DCGAN"],"metadata":{"id":"rp2kAVwsXy1N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Kj3QqiH572d"},"outputs":[],"source":["''' Takes an image as input,it would be real or fake(generated).\n","    this input passed through series of conv,batch norm and activation function\n","    layers for downsampling\n","    gives ouput as whether it is real or fake(0 or 1)\n","'''\n","\n","\n","class dcgan_Discriminator(nn.Module):\n","    def __init__(self, image_channels,dis_features):\n","        super(dcgan_Discriminator, self).__init__()\n","        self.discriminator_layers = nn.Sequential(\n","            nn.Conv2d(in_channels= image_channels,out_channels=dis_features,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.LeakyReLU(0.2,inplace=True),\n","\n","            nn.Conv2d(in_channels=dis_features,out_channels=dis_features*2,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.BatchNorm2d(dis_features*2),\n","            nn.LeakyReLU(0.2,inplace=True),\n","\n","            nn.Conv2d(in_channels=dis_features*2,out_channels=dis_features*4,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.BatchNorm2d(dis_features*4),\n","            nn.LeakyReLU(0.2,inplace=True),\n","\n","            nn.Conv2d(in_channels=dis_features*4,out_channels=dis_features*8,kernel_size=4,stride=2,padding=1,bias=False),\n","            nn.BatchNorm2d(dis_features*8),\n","            nn.LeakyReLU(0.2,inplace=True),\n","\n","            nn.Conv2d(in_channels=dis_features*8,out_channels=1,kernel_size=4,stride=1,padding=0,bias=False),\n","            nn.Sigmoid()#gives 0 or 1\n","        )\n","\n","    def forward(self,x):\n","        return self.discriminator_layers(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObUj_mYY6DEI"},"outputs":[],"source":["#Weights initialization\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxGQCzp05-Ua"},"outputs":[],"source":["#testing the models(Generator and Discriminator)\n","def test_models():\n","    z_dim,k,in_channels,H,W =100,10,3,128,128 #k-examples,we are taking(10)\n","    x=torch.randn(k,in_channels,H,W)\n","    discriminator= dcgan_Discriminator(in_channels,64)\n","    weights_init(discriminator_layers)\n","    assert discriminator_layers(x).shape==(k,1,1,1)\n","    generator=dcgan_Generator(z_dim,in_channels,k)\n","    z=torch.randn((k,z_dim,1,1))\n","    weights_init(generator)\n","    assert generator(z).shape==(k,in_channels,H,W)\n","    print(\"model working..\")\n","\n","if __name__ == \"__main__\":\n","    test_models()"]},{"cell_type":"code","source":["Gen = dcgan_Generator(z_dim, image_channels, gen_features).to(device)\n","Gen.apply(weights_init)\n","\n","Disc = dcgan_Discriminator(image_channels, dis_features).to(device)\n","Disc.apply(weights_init)"],"metadata":{"id":"pPrhr2o5X7II"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.BCELoss()"],"metadata":{"id":"mQJUCiugX8QR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Disc_optimizer = optim.Adam(Disc.parameters(), lr=lr, betas=(0.5, 0.999))\n","Gen_optimizer = optim.Adam(Gen.parameters(), lr=lr, betas=(0.5, 0.999))"],"metadata":{"id":"MQjpOsj0YAny"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90mXXjbh6Us6"},"outputs":[],"source":["\n","Animal_transforms = transforms.Compose(\n","    [\n","        transforms.Resize((128, 128)),\n","        # transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n","        # transforms.RandomVerticalFlip(p=0.5),  # Randomly flip images Vertically\n","        # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Translation augmentation\n","        # transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),  # Brightness, contrast and saturation\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5] * 3, [0.5] * 3),\n","    ]\n",")\n","\n","Animal_dataset= datasets.ImageFolder(root='/content/drive/MyDrive/chaitanya/animals_resized/animals_resized',transform= Animal_transforms)\n","dataloader= DataLoader(Animal_dataset,batch_size,shuffle=True)\n"]},{"cell_type":"code","source":["# print(f\"No.of classes: {len(Animal_dataset.classes)}\")\n","# print(f\"No.of images: {len(Animal_dataset)}\")\n","# print(\"Total Classes:\", Animal_dataset.classes)"],"metadata":{"id":"2RZzz8OlYKTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import datasets, transforms\n","\n","Butterfly_transforms = transforms.Compose(\n","    [\n","        transforms.Resize((128,128)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5] * 3, [0.5] * 3),\n","    ]\n","\n",")\n","\n","Butterfly_dataset= datasets.ImageFolder(root='/content/drive/MyDrive/chaitanya/Butterfly_dataset',transform= Butterfly_transforms)\n","bydataloader= DataLoader(Butterfly_dataset,batch_size,shuffle=True)"],"metadata":{"id":"UVw7DMuYYM29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(f\"No.of classes: {len(Butterfly_dataset.classes)}\")\n","# print(f\"No.of images: {len(Butterfly_dataset)}\")\n","# print(\"Total Classes:\", Butterfly_dataset.classes)"],"metadata":{"id":"LFDb0aSIYNuG"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1vE4RKmnJhW4isehQQowjbtnzoxwM-bgR","timestamp":1729451873671}],"mount_file_id":"1vE4RKmnJhW4isehQQowjbtnzoxwM-bgR","authorship_tag":"ABX9TyPwzDYdfbVoYlpP0cqWYmq0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}